<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>3.2.1.4.4. Reporting Results</title><link rel="stylesheet" href="Common_Content/css/default.css" type="text/css" /><link rel="stylesheet" media="print" href="Common_Content/css/print.css" type="text/css" /><meta name="generator" content="publican 2.5" /><meta name="package" content="Beaker-Deployment_Guide-0.5-en-US-1-9" /><link rel="home" href="index.html" title="Deployment Guide" /><link rel="up" href="User_Guide-Getting_Started-Process-Tests.html" title="3.2.1.4. Tests" /><link rel="prev" href="User_Guide-Getting_Started-Process-Tests-Unassimilated_or_Unfinsihed_Content-Using_the_startup_test_function.html" title="3.2.1.4.3.1. Using the startup_test function" /><link rel="next" href="User_Guide-Getting_Started-Process-Tests-Reporting_Results-Logging_Tips.html" title="3.2.1.4.4.1. Logging Tips" /></head><body><p id="title"><a class="left" href="https://fedorahosted.org/publican"><img src="Common_Content/images/image_left.png" alt="Product Site" /></a><a class="right" href="https://fedorahosted.org/publican"><img src="Common_Content/images/image_right.png" alt="Documentation Site" /></a></p><ul class="docnav"><li class="previous"><a accesskey="p" href="User_Guide-Getting_Started-Process-Tests-Unassimilated_or_Unfinsihed_Content-Using_the_startup_test_function.html"><strong>Prev</strong></a></li><li class="next"><a accesskey="n" href="User_Guide-Getting_Started-Process-Tests-Reporting_Results-Logging_Tips.html"><strong>Next</strong></a></li></ul><div xml:lang="en-US" class="section" id="User_Guide-Getting_Started-Process-Tests-Reporting_Results" lang="en-US"><div class="titlepage"><div><div><h5 class="title" id="User_Guide-Getting_Started-Process-Tests-Reporting_Results">3.2.1.4.4. Reporting Results</h5></div></div></div><div class="para">
		The philosophy of Beaker is that the engineers operating the system will want to quickly survey large numbers of tests, and thus the report should be as simple and clear as possible. "PASS" indicates that everything completed as expected. "FAIL" indicates that something unexpected occurred.
	</div><div class="para">
		In general, a test will perform some setup (perhaps compiling code or configuring services), attempt to perform some actions, and then report on how well those actions were carried out. Some of these actions are your responsibility to capture or generate in your script: 
		<div class="itemizedlist"><ul><li class="listitem"><div class="para">
					a PASS or FAIL and optionally a value indicating a test-specific metric, such as a performance figure.
				</div></li><li class="listitem"><div class="para">
					a debug log of information &amp; mdash;invaluable when troubleshooting an unexpected test result. A test can have a single log file and report it into the root node of your results tree, or gather multiple logs, reporting each within the appropriate child node.
				</div></li></ul></div>

	</div><div class="para">
		Other components of the result can be provided automatically by the framework when in a test lab environment: 
		<div class="itemizedlist"><ul><li class="listitem"><div class="para">
					the content of the kernel ring buffer (from dmesg). Each report clears the ring buffer, so that if your test reports multiple results, each will contain any messages logged by the kernel since the last report was made.
				</div></li><li class="listitem"><div class="para">
					a list of all packages installed on the machine under test (at the time immediately before testing began), including name, version/release, and architecture.
				</div></li><li class="listitem"><div class="para">
					a separate report of the packages listed in the RunFor of the metadata including name, version/release, and architecture (since these versions are most pertinent to the test run).
				</div></li><li class="listitem"><div class="para">
					if a kernel panic occurs on the machine under test, this is detected for you from the console log output, and will cause a Panic result in place of a PASS or FAIL for that test.
				</div></li></ul></div>

	</div><div class="para">
		In addition, the Beaker framework provides a hierarchical namespace of results, and each test is responsible for a subtree of this namespace. Many simple tests will only return one result (the node they own), but a complex test can return an entire subtree of results as desired. The location in the namespace is determined by the value of variables defined in the Makefile. These variables will be discussed in the Packaging section.
	</div><div class="para">
		A test may be testing a number of related things with a common setup (e.g. a setup phase of a server package onto localhost, followed by a collection of tests as a client). Some of these things may not work across every version/architecture combination. This will produce a list of "subresults", each of which could be classified as one of: 
		<div class="itemizedlist"><ul><li class="listitem"><div class="para">
					expected success: will lead to a PASS if nothing else fails
				</div></li><li class="listitem"><div class="para">
					expected failure: should be as a PASS (as you were expecting it).
				</div></li><li class="listitem"><div class="para">
					unexpected success: can be treated as a PASS (since it's a success), or a FAIL (since you were not expecting it).
				</div></li><li class="listitem"><div class="para">
					unexpected failure: should always be a FAIL
				</div></li></ul></div>

	</div><div class="para">
		Given that there may be more than one result, the question arises as to how to determine if the whole test passes or fails. One way to verify regression tests is to write a script that compares a set of outputs to an expected "gold" set of outputs which grants PASS or FAIL based on the comparison.
	</div><div class="para">
		It is possible to write a script that silently handles unexpected successes, but it is equally valid for a script to report a FAIL on an unexpected success, since this warrants further investigation (and possible updating of the script).
	</div><div class="para">
		To complicate matters further, expected success/failure may vary between versions of the package under test, and architecture of the test machine.
	</div><div class="para">
		If the test is checking multiple bugs, some of which are known to work, and some of which are due to be fixed in various successive (Fedora) updates, ensure that the test checks everything that ought to work, reporting PASS and FAIL accordingly. If the whole test is reporting a single result, it will typically report this by ensuring that all expected things work; as bugs are fixed, more and more of the test is expected to work and can cause an overall FAIL.
	</div><div class="para">
		If it is reporting the test using a hierarchy of results, the test can have similar logic for the root node, and can avoid reporting a result for a subtree node for a known failure until the bug is fixed in the underlying packages, and avoid affecting the overall result until the bug(s) is fixed.
	</div><div class="para">
		As a general Beaker rule of thumb, a FAIL anywhere within the result subtree of the test will lead to the result for the overall test being a FAIL.
	</div></div><ul class="docnav"><li class="previous"><a accesskey="p" href="User_Guide-Getting_Started-Process-Tests-Unassimilated_or_Unfinsihed_Content-Using_the_startup_test_function.html"><strong>Prev</strong>3.2.1.4.3.1. Using the startup_test function</a></li><li class="up"><a accesskey="u" href="#"><strong>Up</strong></a></li><li class="home"><a accesskey="h" href="index.html"><strong>Home</strong></a></li><li class="next"><a accesskey="n" href="User_Guide-Getting_Started-Process-Tests-Reporting_Results-Logging_Tips.html"><strong>Next</strong>3.2.1.4.4.1. Logging Tips</a></li></ul></body></html>
